---
title: "[3D Vision] Fundamentals of Gaussian Splatting"
date: 2025-02-12 21:35:00 +09:00
categories: [Computer Vision, 3D Vision]
tags:
  [
    3D Vision
  ]
---

[Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review 논문 링크](https://ieeexplore.ieee.org/abstract/document/10545567)
3D Vision의 다양한 분야를 학습하는 중이다. 오늘은 Gaussian Splatting.

---

# A Primer on 3D reconstruction and novel view synthesis

### Traditional Approaches

- 포인트 클라우드
    - 각 포인트가 (x, y, z)라는 3차원 튜플로 나타내어짐
- 메시
    - 3D 포인트들의 집합인 정점(vertice)들로 구성 →이 정점들을 기반으로 정의된 다각형 집합인 면(faces)이 포함됨.
- 복셀
    - 3D 부피를 작은 큐브로 나누어 정의→각 개별 큐브를 복셀이라함.

![image.png](assets/img/250404post/image.png)

### Novel Approaches

- **신경망/다층 퍼셉트론(MLP):**
    - NeRFs(Neural Radiance Fields): 3D 장면을 표현할 때 전통적인 3D 기하학적인 방법과 신경망을 결합. → 즉, 3D 공간에서 각 점에 대한 정보를 신경망을 통해 처리하여, 복잡한 빛의 반사, 굴절, 그림자 등을 정교하게 다룸.
        - "radiance field"라는 신경망 = 3D 공간 내의 특정 위치(좌표)를 입력받아 해당 위치에서 빛의 색상(그 지점의 표면이 어떤 색을 띠는지)과 법선 벡터(그 지점에서 표면이 어떻게 기울어져 있는지) 출력. → 여러개의 이미지 데이터를 학습해 3D 공간에서 어떤 위치에 어떤색과 표면이 있는지 알게됨.
- **Gaussian Splats:**
    - Gaussian splats는 3D 장면의 각 부분을 **Gaussian 함수**를 사용해 표현 → 각 3D 점이 주변 환경에 미치는 영향을 부드럽게 처리. 이 함수는 점의 크기, 색상, 밀도 등을 결정하며, 이를 **매개변수 집합**으로 표현.
        - “splat” = 일종의 3D 공간에서 점이나 작은 영역을 나타내는 요소
        - 매개변수 집합
            - **위치:** Splat의 중심 3D 위치 (x,y,z)
            - **스케일:** Splat의 크기. (큰 splat은 주변의 더 넓은 영역을 표현하고, 작은 splat은 더 세밀한 영역)
            - **불투명도:** Splat이 렌더링된 이미지에 미치는 영향의 정도.
            - **색상:** Splat의 색상.
            - **재료 속성:** 광택, 반사 및 굴절과 같은 추가적인 속성.
    - 이처럼 splat을 간결한 형식으로 저장함으로써 상대적으로 적은 데이터로 많은 수의 splat을 표현할 수 있음. voxel에 비해 유리한 메모리.

![image.png](assets/img/250404post/image1.png)

# Fundamentals of Gaussian Splatting

![image.png](assets/img/250404post/image2.png)

{:.prompt-block}
>1. 3D 장면의 포인트 클라우드를 얻는다. (COLMAP과 같은 SFM을 이용)
>2. 포인트를 Gaussian 형태로 변환한다. (각 3D 점을 중심으로 Gaussian 형태(위치, 색상, 크기, 불투명도)를 부여한다. )
>3. 가우시안을 카메라의 시점에서 2D 로 투영한다. (3D 가우시안을 2D 화면상에 나타낸다)
>4. Depth compositing을 통해 투영된 가우시안을 합성한다. (여러개의 가우시안이 화면상에 겹쳐서 나타날 때, 어느 가우시안이 앞에 있는지, 즉 카메라에 가까운지에 따라 정확히 순서대로 합성해야함.)

1. **구조에서 움직임을 추적(SFM):**
    
    **SFM(Structure from Motion)** 방법을 사용하여 이미지를 통해 **COLMAP** 라이브러리를 이용해 포인트 클라우드를 생성하는 것으로 시작.
    
    - SFM
        - **여러 이미지 수집:** 동일한 장면을 여러 각도에서 촬영한 2D 이미지
        - **특징점 추출**
        - **특징점 매칭:** 여러 이미지들에서 같은 특징점을 찾아 매칭
        - **3D 구조 계산:** 두 이미지에서 대응되는 점들을 찾으면, 이들 간의 관계를 이용하여 3D 구조 추정. =삼각 측량 (Triangulation). 두 이미지에서 동일한 점이 어디에 있는지 알면, 그 점이 3D 공간에서 어디에 위치하는지 계산할 수 있음.
        - **카메라 위치 추정:** 각 이미지가 촬영된 카메라의 위치와 방향도 추정. (이미지 내의 특징점 위치를 이용해 카메라의 이동 경로를 추적가능). → 카메라의 **외적(extrinsic parameters)**, 즉 위치와 회전 정보를 계산하는 과정
        - **최적화:** 최소제곱법 등 최적화 방법을 써서 특징점들의 위치와 카메라의 위치를 정확하게 추정
2. **Gaussian Splatting으로 변환:**
    
    각 점을 Gaussian Splat으로 변환하여 래스터화를 가능하도록.
    
    - 래스터화(rasterization) : 3D 모델과 같은 벡터 그래픽을 픽셀로 구성된 2D 이미지로 변환하는 과정. → 3D 모델을 화면에 표시하려면 3D 공간의 정점과 면 정보를 2D 화면의 픽셀로 변환해야함.
    - Gaussian Splatting에서 각 점을 가우시안으로 변환하는 것은 3D 씬을 래스터화하기에 적합한 형태로 표현하는 것을 의미함. 즉, 각 가우시안은 2D 이미지의 픽셀에 색상 값을 "뿌리는(splatting)" 역할을 하며, 이러한 방식으로 3D 씬을 2D 이미지로 변환할 수 있음. SFM 데이터만으로는 각 점의 크기, 방향, 불투명도 등의 정보를 알 수 없기 때문에 각 점을 가우시안으로 표현함으로써 래스터화 가능.
3. **Training 학습 :**
    - 신경망과 유사하게 Stochastic Gradient Descent(SGD, 확률적 경사 하강법)가 사용됨
    - 미분 가능한 Gaussian rasterization을 사용하여 이미지 내에서 Gaussian을 래스터화.
    - 래스터 이미지와 실제 지형 이미지 간의 차이를 기반으로 loss 계산
    - 발생한 손실에 따라 Gaussian 파라미터를 수정
4. **Differentiable Gaussian Rasterization 미분 가능한 가우시안 래스터화:**
    
    2D 가우시안은 카메라 시점에서 투영되고, 깊이에 따라 정렬되며, 모든 픽셀에 대해 앞뒤로 반복되는 미분가능한 가우시안 래스터화를 거침. 
    
    {:.prompt-tip}
    
    >왜 미분 가능해야하냐?
    >- 래스터화 과정이 미분 불가능하면, 손실함수의 기울기 계산 불가능 → 경사하강법 같은 최적화 알고리즘 못씀. → 가우시안 스플래팅 학습 불가능.
    
    
    - 2D 가우시안은 카메라 시점에서 투영되어야 함.
    - 깊이에 따라 정렬되어야 함
    - 모든 픽셀에 대해 앞뒤로 반복되어 결합됨
    - 이러한 과정은 미분 가능한 가우시안 래스터화 필요.
    - 여기서 미분 가능하다는 것은 래스터화 과정이 optimization에 사용될 수 있도록 파라미터에 대해 미분 가능하다는 의미

### Mathmatical Representation and Rendering Process

하나의 3D 가우시안(Gaussian)은 다음과 같은 매개변수로 표현된다.

- **평균(mean)**: $\mu \in \mathbb{R}^3$
    
    (3차원 공간에서의 중심 위치를 의미함)
    
- **공분산(covariance)**: $\Sigma \in \mathbb{R}^{3 \times 3}$
    
    (가우시안의 형태와 크기를 나타냄)
    
    →**공분산 = 변수 간 관계의 방향과 크기를 나타냄.** 공분산 값이 양수면 두 변수가 함께 증가하거나 함께 감소. 공분산 값이 음수면 한 변수가 증가할 때 다른 변수는 감소. 공분산 값이 0에 가까우면, 두 변수는 거의 관계x
    
    → 공분산 행렬 = 변수가 2개 이상일때, 변수 사이의 모든 관계(공분산)을 한 번에 나타내는 것. (변수가 3개이면 3X3)
    
    → 여기에서 변수가 크기, 방향, 모양 같은 것이 아니고 공분산 행렬의 값을 통해 종합적으로 결정되는게 크기, 방향, 모양인것임. 
    
    - 즉, 크기, 방향, 모양은 이 공분산 행렬의 변수들이 종합적으로 결정하는 요소.
    - 정확히 말하면, 공분산 행렬은 **6개의 독립적 매개변수**를 가지고 있습니다:
        - 대각 성분 3개$\sigma_{xx}, \sigma_{yy}, \sigma_{zz}$
        - 비대각 성분 3개$\sigma_{xy}, \sigma_{xz}, \sigma_{yz}$
    
    이 6개의 값으로 가우시안의 **모양(shape)**, **크기(size)**, **방향(orientation)** 모두가 결정됨. (ex. 대각성분이 작고 비대각 성분이 0이면 작은 구형태)
    
- **색상(color)**: $c \in \mathbb{R}^3$
    
    (가우시안 스플랫의 RGB 색상 값)
    
- **불투명도(opacity)**: $o \in \mathbb{R}$
    
    (가우시안이 최종 이미지에 미치는 투명도를 의미함)
    

가우시안을 렌더링(rendering)하여 화면상에서 보기 위해서는, 우선 **각 가우시안을 카메라 평면으로 투영하여** 2차원 위치와 그 범위(extent)를 계산함.

이후 **보이는(화면에 나타나는) 2D 가우시안들을 카메라와의 거리(깊이)에 따라 정렬한 후, 가장 가까운 것부터 먼 것 순서로 합성(composite)** 하여 최종 이미지를 생성함.

즉, 가까운 가우시안이 먼 가우시안 위에 덧입혀지는 방식으로 렌더링이 이루어짐.

**Projection of Gaussians**

- **카메라의 외부 파라미터 변환 행렬**  $T_{cw}$
    
    $T_{cw} = \begin{bmatrix}
    R_{cw} & t_{cw} \\
    0 & 1
    \end{bmatrix} \in SE(3),$
    
    - $T_{cw}$는 월드 좌표계를 카메라 좌표계로 변환하는 외부 파라미터(Extrinsics)를 나타내는 4x4 동차 변환 행렬
        - 월드 좌표계 : 현실 세계의 기준이 되는 좌표계
        - 카메라 좌표계 : 카메라의 위치를 기준으로한 좌표계
    - $R_{cw}$ 는 3x3 회전 행렬로, 월드 좌표계에서 카메라 좌표계로의 회전
    - $t_{cw}$ 는 3차원 병진 벡터로, 월드 좌표계에서 카메라 좌표계로의 이동
    - $SE(3)$는 특수 유클리드 그룹으로, 3차원 공간에서의 강체 변환(회전 및 이동)
    
    → 3D 점이 월드 좌표에서 카메라의 눈(좌표계)를 기준으로 어디에 위치하는지 계산하는 행렬
    
- **투영 행렬 P**
    
    $P = \begin{bmatrix}
    \frac{2 f_x}{w} & 0 & 0 & 0 \\
    0 & \frac{2 f_y}{h} & 0 & 0 \\
    0 & 0 & \frac{f+n}{f-n} & -\frac{2 f_n}{f-n} \\
    0 & 0 & 1 & 0
    \end{bmatrix}$
    
    - P 는 카메라 좌표계의 3D 점을 정규화된 클립 공간(Normalized Device Coordinates, NDC)으로 투영하는 4x4 투영 행렬.
    - $f_x$ 와  $f_y$ 는 각각 x축과 y축 방향의 초점 거리.(얼마나 줌되어있는가)
    - $c_x$ 와  $c_y$ 는 이미지 평면의 주점
    - $w$ 와  $h$ 는 출력 이미지의 너비와 높이.
    - $n$ 과  $f$ 는 각각 near(최소) 및 far(최대) 클리핑 평면 (카메라가 볼 수 있는 최대 최소 거리 범위)
    
    → 카메라가 보는 화면에서 3D 장면을 2D 이미지로 바꿔주는 행렬
    
- **3D 평균 $\mu$ 의 투영**
    
    $t = T_{cw} \begin{bmatrix} \mu \\ 1 \end{bmatrix}, \quad
    t' = Pt, \quad
    \mu' = \begin{bmatrix}
    \frac{1}{2} \left( w\frac{t'_x}{t'_w} + 1 \right) + c_x \\
    \frac{1}{2} \left( h\frac{t'_y}{t'_w} + 1 \right) + c_y
    \end{bmatrix}$
    
    - 3D 가우시안 분포의 평균 $\mu$를 픽셀 좌표계로 투영하는 과정
    - 먼저 3D 점 $\mu$ 를  $T_{cw}$를 사용하여 카메라 좌표계로 변환 → $t$
    - 그 후, 투영 행렬 P를 사용하여 정규화된 장치 좌표(NDC)로 변환. → $t'$ .
    - 최종적으로 픽셀 좌표  $\mu'$ 로 변환.
    
    → 픽셀좌표$\mu'$는 실제 화면상에서 그 점이 어디에 보이는지를 나타냄.
    
- **아핀 변환 행렬 J**
    
    $J = \begin{bmatrix}
    \frac{f_x}{t_z} & 0 & -\frac{f_x t_x}{t_z^2} \\
    0 & \frac{f_y}{t_z} & -\frac{f_y t_y}{t_z^2}
    \end{bmatrix}$
    
    - 카메라 좌표게에서 1차 테일러 전개를 사용하여 공분산 $\Sigma$ 를 2D 픽셀 공간으로 변환하는 행렬
    - $f_x$ 와 $f_y$ 는 초점 거리,  $t_x , t_y ,  t_z$ 는 카메라 좌표계에서 점 t 의 좌표
    
    → 카메라 시점에서 공분산을 정확하게 화면에게 투영하기 위해 필요. 실제 3D 형태가 화면에서 얼마나 늘어나고 줄어드는지를 결정
    
- **2D 공분산 행렬  $\Sigma'$**
    
    $\Sigma' = J R_{cw} \Sigma R_{cw}^\top J^\top$
    
    - 3D 공분산 행렬 $\Sigma$ 를 화면상에서 나타나는 2D 공분산 행렬  $\Sigma'$ 로 변환
    - $R_{cw}$: 월드 좌표계에서 카메라 좌표계로의 회전 행렬
    - $J$ : 앞서 계산한 아핀 행렬. 최종적으로 화면에서 가우시안의 크기와 형태 결정됨.
- **회전 행렬 $R$**
    
    $R = \begin{bmatrix}
    1 - 2(y^2 + z^2) & 2(xy - wz) & 2(xz + wy) \\
    2(xy + wz) & 1 - 2(x^2 + z^2) & 2(yz - wx) \\
    2(xz - wy) & 2(yz + wx) & 1 - 2(x^2 + y^2)
    \end{bmatrix}$
    
    - 쿼터니언 $q = (x, y, z, w)$ 를 사용하여 3D 회전 행렬  R 을 구성
    - 쿼터니언은 3D 공간에서 회전을 표현하는데, 짐벌락(회전불가)이 없고 안정적
- **3D 공분산 행렬** $\Sigma$
    
    $\Sigma = R S S^\top R^\top$
    
    - 스케일 $s$와 회전행렬 R (크기, 방향)을 사용하여 3D 가우시안의 형태 표현.

3D 공간의 가우시안을 카메라가 보는 화면상에서 올바른 위치와 크기, 형태로 나타내기 위한 과정

**Depth Compositing of Gaussians**

화면상에서 여러개의 가우시안이 겹쳤을 때, 이들을 depth 순서대로 정확하게 합성해서 최종 색상을 만드는 과정. 

1. **99% 신뢰 타원을 이용한 Bounding Box 설정**
    
    각 가우시안은 2D 화면상에서 타원형 형태. → 공분산 $\Sigma'$으로 결정됨.
    
    - 99% 신뢰 타원 = 가우시안의 중심에서 약 3$\sigma$범위까지 확장된 영역 (가우시안의 영향이 거의 대부분 나타나는 영역)
    
    이 타원을 포함하는 가장 작은 직사각형 = Bounding Box 
    
    {:.prompt-tip}
    
    >왜 Bounding Box를 사용할까?
    >- 화면에 수많은 픽셀이 있는데 모든 픽셀마다 가우시안의 영향을 계산하면 너무 느림.
    >- 영향이 거의 없는 먼 픽셀 빼고, 영향이 있는 근처 픽셀들만 빠르게 처리하기 위해 Bounding Box 사용
    
    
2. **타일에 따른 가우시안 배정**
    
    화면을 더 빠르게 처리하기 위해 보통 전체 화면을 작은 타일들로 나눔.
    
    - 위에서 구한 Bounding Box가 특정 타일과 조금이라도 겹치면, 그 타일의 리스트에 해당 가우시안 추가
    - 각 타일은 화면의 특정 영역이며, 각 타일마다 그 타일과 겹치는 가우시안 리스트가 생성됨.
    
    이제 각 타일은 겹쳐진 가우시안 목록을 가짐.
    
3. **타일 내에서 가우시안 Depth 순서로 정렬**
    - 타일에 포함된 가우시안들을 깊이 순으로 정렬.
    - 앞에 있는 (가까운) 가우시안부터 순차적으로 화면 픽셀의 색상에 영향을 미치도록 함.
4. **Rasterization (래스터화, 픽셀화)**
    - 각 타일에 정렬된 가우시안들을 실제 화면 픽셀로 나타내는 과정.
    - 가우시안의 색상, 불투명도를 고려함.
5. **픽셀 색상의 최종 계산식**
    
    픽셀 i의 최종 색상 $C_i = \sum_{n \leq N} c_n \alpha_n T_n$
    
    - N: 픽셀 i에 영향을 주는 가우시안의 개수
    - $c_n$: n번째 가우시안의 색상
    - $\alpha_n$: n번째 가우시안의 불투명도
    - $T_n$: n번째 가우시안까지의 누적된 투과율(transmittance)
    
    투과율 $T_n = \prod_{m<n}(1-\alpha_m)$
    
    - $T_n$은 "n번째 가우시안 이전의 모든 가우시안의 투명도를 누적한 값"
    - 즉, 앞에 있는 가우시안이 완전히 불투명하면, 뒤의 가우시안은 전혀 보이지 않음.
6. **각 가우시안의 불투명도(Opacity, α) 계산**
    
    불투명도 $\alpha_n$는 각 픽셀과 가우시안의 거리에 따라 결정됨.
    
    $\alpha_n = o_n\exp(-\sigma_n), \quad \sigma_n = \frac{1}{2}\Delta_n^\top \Sigma'^{-1}\Delta_n$
    
    - $o_n$: 가우시안의 기본 불투명도 (가운데일수록 최대)
    - $\Delta_n$: 픽셀의 중심과 가우시안 중심 간 거리(offset)
    - $\Sigma'$: 가우시안의 형태를 나타내는 2D 공분산 행렬
    
    쉽게 말하면, 가우시안 중심에 가까울수록 불투명도가 높아지고, 중심에서 멀어질수록 불투명도가 낮아지는 형태.
    

### Quality Assessment Matrices

가우시안 스플래팅 품질평가 = PSNR, SSIM, LPIPS.

- PSNR(Peak Signal to Noise Ratio)
    
    $PSNR(I) = 10 \cdot \log_{10} \left( \frac{MAX(I)^2}{MSE(I)} \right)$
    
    - 이미지 최대 신호대 잡음비. 이미지 품질을 평가하는데 사용되는 reference 지표
    - 최대 신호(MAX(I)는 픽셀 값이 가질 수 있는 최대값(8비트 이미지라면 255)
- SSIM(Structural Simiarity Index Measure)
    
    $SSIM(x, y) = \frac{(2 \mu_x \mu_y + C_1)(2 \sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}$
    
    - 이미지의 구조적 유사성을 측정하는 full-reference 지표
        - $\mu_x$, $\mu_y$: 이미지 패치 x와 y의 평균.
        - $\sigma_x^2$, $\sigma_y^2$: 이미지 패치 x와 y의 분산.
        - $\sigma_{xy}$: 이미지 패치 x와 y의 공분산.
        - $C_1= (K_1 L)^2, C_2 = (K_2 L)^2$: 안정성을 위한 상수, L은 픽셀의 동적 범위 (8비트 정수의 경우 255), $K_1 = 0.01, K_2 = 0.03.$
    
    → PSNR과 달리 인간의 시각적 특성을 고려해 이미지의 구조, 밝기, 대비의 유사도를 측정함.
    
- LPIPS(Learned Perceptual Image Patch Similarity)
    
    $\text{LPIPS}(x, y) = \sum_{l}^{L} \frac{1}{H_l W_l} \sum_{h,w}^{H_l,W_l} \| w_l (\varsigma)(x_{hw}^{l} - y_{hw}^{l}) \|_{2}^{2}$
    
    - 학습된 컨볼루션 특징을 활용하는 **완전 참조(full-reference)** 방식의 이미지 품질 평가 지표
        - $x_{hw}^{l},y_{hw}^{l}$: 픽셀 위치 (너비 w, 높이 h), 레이어(l)에서의 원본 이미지(x) 및 생성된 이미지(y)의 특징(feature)
        - $W_l,H_l$: 해당 레이어(l)에서의 특징 맵(feature map)의 너비와 높이
        - $w_l(\varsigma)$: 학습된 가중치(특징 맵 사이의 유사도 계산 시 사용되는 가중치)
    
    → CNN 네트워크에서 나온 특징맵을 이용해서, 픽셀 간 차이가 아닌 **특징의 차이**를 계산하여 점수를 냄. 점수가 낮을수록 두 이미지가 더 유사하다는 의미.
    
