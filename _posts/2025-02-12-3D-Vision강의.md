---
title: "[CV-pedia] ECCS 498 Lecture 17: 3D Vision "
date: 2025-02-12 21:35:00 +09:00
categories: [Computer Vision, 3D Vision]
tags:
  [
    3D Vision
  ]
---
[Lecture 17 : 3D Vision 유튜브 강의 링크](https://www.youtube.com/watch?v=S1_nCdLUQQ8)

[강의 ppt 링크](https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture17.pdf)

---

3D Vision 입문 전, 위 강의를 통해 전체적인 개념을 훑고 간다.

## 1. 3D Vision Topics

Neural Network가 3D 정보를 예측하는 방법 뿐만 아니라 3D 데이터를 입력으로 활용하는 방법에 대해 살펴본다. 

![image.png](assets/img/250212post/image.png)

- Predicting 3D Shapes from single image
    - input : single RGB image
    - output : representation of 3D Shape(해당 이미지 속 객체의 3D 형태)
- Processing 3D input data
    - input : 3D data
    - classification, segmentation 등의 작업 수행

(모두 fully supervised learning으로 가정)

3D Vision의 다양한 주제들 

![image.png](assets/img/250212post/image1.png)

## 2. 5가지 3D 형태 표현 방식

### 2.1 Depth Map

![image.png](assets/img/250212post/image2.png)

![image.png](assets/img/250212post/image3.png)

- **개념**: 입력 이미지의 각 픽셀에 대해 카메라와의 거리를 할당하는 3D 표현 방식.
- **구조**: RGB 이미지처럼 2D 격자 구조를 가지지만, 색상 대신 각 픽셀의 깊이(depth) 정보를 포함.
- **2.5D 표현**:
    - 완전한 3D가 아니라 가시 영역만 표현 가능.
    - 가려진(occluded) 객체의 깊이 정보를 캡처할 수 없음.

![image.png](assets/img/250212post/image4.png)

- **RGB 이미지에서 Depth Map 예측**
    - 완전 합성곱 네트워크(FCN) 활용하여 특징 추출 및 복원(다운샘플링 → 업샘플링)후 마지막 합성곱 층에서 1채널(depth) 출력

![image.png](assets/img/250212post/image5.png)

- **문제점: 스케일-깊이 모호성(Scale-Depth Ambiguity)**
    - 같은 크기의 물체가 멀리 있거나 작은 물체가 가까이 있어도 동일한 2D 투영을 가짐 → 절대 깊이 예측 어려움.

![image.png](assets/img/250212post/image6.png)

- **해결책: 스케일 불변 손실 함수(Scale-Invariant Loss Function)**
    - 예측된 깊이 값이 일정한 스케일 팩터를 곱했을 때 실제 깊이와 일치하면 손실 값 0.
    - 절대 깊이가 아니라 상대적인 깊이 관계를 학습.

**표면 법선 맵(Surface Normal Map)**

![image.png](assets/img/250212post/image7.png)

- **개념**: 각 픽셀에 대해 해당 픽셀이 나타내는 객체의 표면 방향(법선 벡터, normal vector)을 저장하는 3D 표현 방식.
- **깊이 맵과 차이점**:
    - 깊이 맵: 픽셀별 카메라까지의 거리(깊이) 저장.
    - 표면 법선 맵: 픽셀별 표면 방향(벡터)을 (x, y, z) 형태로 저장.
    - 출력층에서 3채널(각 픽셀의 법선 벡터)를 예측
- **RGB 색상을 활용한 시각적 표현**:
    - 파란색(Blue): 위쪽(↑) 방향 → 바닥, 침대 상단
    - 빨간색(Red): 오른쪽(→) 방향 → 침대 측면
    - 녹색(Green): 왼쪽(←) 방향 → 캐비닛 측면

![image.png](assets/img/250212post/image8.png)

- **한계**:
    - 가려진(occluded) 객체의 표면 방향을 복원할 수 없음.
    - 완전한 3D 표현(점군, 3D 메쉬 등)이 필요할 수도 있음.

### 2.2 Voxel Grid

![image.png](assets/img/250212post/image9.png)

![image.png](assets/img/250212post/image10.png)

- **개념:** 3D 세계를 3D 격자(Grid) 로 표현하는 방식.
    - 각 격자의 셀(Cell)이 켜짐(On) 또는 꺼짐(Off) 상태로 표현됨 → 셀이 차지된(occupied) 상태인지 아닌지를 나타냄.
    - 마인크래프트와 유사 → 세계를 블록 단위로 구성.
- **특징:**
    - 2D에서 Mask R-CNN이 객체의 Foreground과 Background을 구별하는 것과 유사.
    - 2D 점유 격자(Occupancy Grid)나 Segmentation 처리하는 방법과 유사하게 3D Voxel Occupancy Grid에도 적용 가능
- **문제점:**
    - 해상도가 낮으면 디테일 손실
        - 예시: 의자의 미세한 곡선이나 디테일이 낮은 해상도에서는 블록 형태로 표현되어 손실됨.
    - 고해상도가 필요하면 계산 비용 증가
        - 세밀한 구조(Geometry)를 표현하려면 매우 높은 해상도가 필요 → 연산량 증가로 비용(Computational Cost) 이 커짐.
- **객체 분류(Classification)** 가능
    - 주어진 Voxel Grid가 의자인지, 비행기인지, 소파인지 분류하는 작업 가능.
    - 3D CNN 구조 사용
        
        ![image.png](assets/img/250212post/image11.png)
        
    - Input : 원본 Voxel Grid(3D 공간의 모든 지점에 대해 그 지점이 점유되었는지 아닌지 정보를 제공하는 데이터)
    - 각 layer는 3D 합성곱 연산을 수행. 이때 커널은 3D 큐브 형태.
    - 3D 커널(Convolutional Kernel)을 사용하여 특징을 추출하고, Fully Connected Layer 또는 Global Average Pooling layer 을 거쳐 최종Classification 수행.
        - 입력 : 4차원 텐서 = 3D 공간 차원 + 1개의 채널 :  1*(30*30*30)
        - 네트워크가 진행되면서 3D 공간 크기는 줄고 Feature Dimension이 증가 : 48*(13*13*13)
    
    ![image.png](assets/img/250212post/image12.png)
    
    - **Fully Connected Layer를 사용한 방법**
        - 한 가지 일반적인 방법은 **완전 연결층(Fully Connected Layer)** 을 이용하여 2D → 3D 변환을 수행하는 것
        - 입력 : 2차원 이미지(높이, 너비, RGB 채널) 로 구성된 3D 텐서
        - 출력 : 3D Voxel Grid(높이, 너비, 깊이, 점유 확률) 를 나타내는 4D 텐서
        1. **입력 이미지를 2D CNN을 통해 처리**
            - 입력 이미지는 일반적인 2D CNN을 통과하여 특징을 추출.
            - CNN의 출력은 (H, W, C) 크기의 **3D 텐서** 가 됨.
        2. **FC Layer를 활용하여 4D 텐서로 변환**
            - CNN의 출력을 벡터(Vector) 형태로 변환 (Flatten).
            - 몇 개의 완전 연결층(FC Layers)을 통과한 후 다시 **4D 텐서** 형태로 변환(Reshape).
            - 이 과정에서 추가적인 공간 차원(Depth)을 생성 하여 2D → 3D 변환 수행.
        3. **3D 합성곱과 업샘플링(Up-sampling) 적용**
            - 변환된 3D 텐서에 3D 합성곱(3D Convolution) 및 3D 업샘플링(3D Up-sampling) 을 적용.
            - 모델의 마지막 부분에서 3D 해상도를 증가시켜 최종적인 Voxel Grid 예측.
            - 이 과정은 세그멘테이션(Semantic Segmentation) 네트워크의 업샘플링 과정과 유사 함.
    - **문제점:**
        - 3D 합성곱 연산은 **매우 높은 계산 비용 요구**.
        - 3D 합성곱은 공간 크기가 증가할수록 연산량이 **세제곱(Cubic)으로 증가** 하므로, 계산량이 급격히 증가함.
        
        ![image.png](assets/img/250212post/image13.png)
        
    - Voxel Tube Representation
        
        2D CNN 만을 사용하여 3D Voxel Grid를 예측하는 방법
        
        1. **입력 이미지(2D CNN 처리)**
            - 입력 : (H, W, 3) 크기의 RGB 이미지
            - 기존의 2D CNN을 통해 **2D 특징 맵** 을 생성 (C, H, W)
        2. **최종 2D CNN 층에서 Voxel Grid 형태로 변환**
            - 최종적인 2D CNN 층의 크기를 **V × V로 설정** (여기서 V는 최종 Voxel Grid 크기)
            - CNN의 출력 채널 수를 **V로 설정** 하여, 출력의 채널 차원(Channel Dimension)을 깊이(Depth) 차원으로 해석
        3. **손실 함수 적용 및 학습**
            - CNN의 최종 출력은 Voxel Grid의 깊이(Depth) 차원 을 나타내는 것
            - 모델은 채널 차원을 Voxel Grid의 깊이(Depth)로 해석하여 손실 계산
            - 이 방법을 사용하면 2D CNN만을 활용할 수 있어 3D CNN보다 계산량이 훨씬 감소됨.
    - **문제점 :**
        - 3D 합성곱을 사용한 모델과 달리, Z 방향(깊이)에서의 이동(Translation)에 대한 불변성(Invariance)을 잃게 됨.
        
        {:.prompt-block}
        > **이동 불변성(Translation Invariance)**은 입력 데이터가 공간적으로 이동(고양이가 왼쪽 하단 or 오른쪽 상단)하더라도, 신경망의 출력이 크게 변하지 않는 성질
        
       
        
        - CNN이 XY 평면에서는 이동 불변성(Translational Invariance)을 가지지만, Z 축(깊이) 방향에서는 이동 불변성을 가지지 않음
            - 의자가 Z축 방향(깊이)으로 이동했을 때 신경망이 같은 객체로 인식하지 못할 수 있다는 점!
            - 예를 들어, 의자가 카메라에 가까워지거나 멀어질 때(Voxel Grid에서 Z축 이동), 3D CNN은 동일한 객체로 인식할 수 있지만,
            - Voxel Tube Representation 방식에서는 그 깊이(Depth)마다 별도의 필터를 학습해야 하기 때문에 일반화가 어려움.
    
    ![image.png](assets/img/250212post/image14.png)
    
- **Voxel Grid 의 메모리 문제**
    - 높은 해상도로 복셀 그리드를 표현하려면 많은 메모리가 필요함. 예를 들어, 1024×1024×1024 해상도의 복셀 그리드를 저장하는 데만 약 4GB의 메모리가 요구.
    - **효율성 문제 해결**: 복셀의 메모리 사용량을 줄이기 위해
        1. **옥트리(Octree)를 이용한 다중 해상도(Multi-Resolution) Voxel Grid**
            
            ![image.png](assets/img/250212post/image15.png)
            
            - 옥트리(Octree) 방식은 Voxel Grid를 다중 해상도(Multi-Resolution)로 표현하는 기법
            - 기본 아이디어는, 객체의 전체적인 형태(Coarse Structure)는 낮은 해상도로 표현하고, 세부적인 부분(Fine Details)만 높은 해상도로 표현 하는 것
            - 예를 들어:
                - 큰 구조는 32³ 해상도의 Voxel Grid로 표현
                - 세부적인 부분만 64³ 또는 128³ 해상도로 추가 저장
            - 이렇게 하면 Voxel Grid의 전체 크기를 줄이면서도 중요한 디테일을 유지할 수 있음.
            - 하지만, 여러 해상도를 혼합하고 희소 데이터(Sparse Data)를 다루는 것이 구현하기 어려움.
        
        **2. 중첩 형태 계층(Nested Shape Layer) 방식**
        
        ![image.png](assets/img/250212post/image16.png)
        
        - Voxel Grid 전체를 밀집(Dense) 형태로 저장하는 대신, 내부에서 바깥쪽으로 형상을 구성하는 방식
        - 러시아의 마트료시카(겹겹이 쌓인 인형)처럼, 객체의 외곽을 큰 레이어(Coarse Layer)로 나타내고, 내부를 더 세밀한 레이어로 표현
        - 방법:
            - 바깥쪽부터 큰 양(Positive) Voxel Layer 생성
            - 그 내부에는 음의(Negative) Voxel Layer 추가
            - 다시 내부에는 더 작은 양의(Positive) Voxel Layer 추가
            - 이런 식으로 Voxel Grid를 희소(Sparse)하게 저장하여 전체적인 표현력을 유지
        - 이 방식은 Voxel Grid를 효율적으로 저장할 수 있도록 해, 고해상도로 확장하는 데 유용

### 2.3 Implicit Surface

![image.png](assets/img/250212post/image17.png)

- 개념 : 3D 형상을 함수로 나타내는(암시적으로 표현하는) 방식 (3D 공간의 임의의 좌표를 입력받아 그 지점이 객체에 의해 점유되었는지 아닌지를 확률로 출력하는 함수를 학습하는 것)

![image.png](assets/img/250212post/image18.png)

- 특징 :
    - 암시적 함수는 3D 공간 내 임의의 점을 입력받으면 그 점이 객체 내부인지 외부인지 판별 가능.
    - 객체의 실제 외부 표면은 **점유 확률이 1/2이 되는 점들의 집합(레벨셋, level set)** 으로 정의(흰색)
    - 암시적 함수의 값을 색상으로 표현하면, 파란색은 값이 1에 가까운 경우(객체 내부), 빨간색은 값이 0에 가까운 경우(객체 외부)를 나타냄
- 서명 거리 함수(Signed Distance Function, SDF)
    - 특정 3D 공간 내 점이 객체의 표면까지의 **유클리드 거리(Euclidean Distance)** 를 반환하는 함수이다.
        - 객체 내부에 있는 경우 → 양수
        - 객체 외부에 있는 경우 → 음수

![image.png](assets/img/250212post/image19.png)

- 신경망을 이용한 학습
    - 3D 좌표를 입력으로 받아 해당 점이 객체 내부인지 외부인지 확률로 출력하는 신경망
        1. 먼저 일정한 격자(grid)로 이 함수를 샘플링하여 각 점이 내부인지 외부인지 판단
        2. 내부와 외부의 경계가 되는 점들을 찾은 후, 다시 더 세밀하게 새로운 지점을 샘플링하여 경계를 정밀하게 추출
        3. 3D 형상의 표면을 복원
    - 문제점 :
        - SDF 신경망 구조를 이미지 데이터와 어떻게 연결할지?
        - 학습된 SDF에서 실제 3D 형상을 추출하는 정확한 알고리즘은 뭔지?

### 2.4 Pointcloud

![image.png](assets/img/250212post/image20.png)

- 개념 : 3D 형상을 3D공간 내의 점들의 집합으로 나타내는 방법. 표현하고자 하는 3D 형상의 표면을 덮는 방식으로 여러개의 점을 3D 공간에 배치하는 것.

![image.png](assets/img/250212post/image21.png)

- 장점 :
    - 높은 적응성 → 특정 부분에 점의 밀도를 다르게 조절하여 세밀한 부분을 더 정밀하게 표현 가능. ( 복셀 그리드는 높은 해상도로 3D 형상을 표현하려면 엄청 높은 복셀 해상도가 필요했었음)
    - 제한된 수의 점만으로도 효율적으로 3D 형상을 표현할 수 있음. (단순한 부분 : 적은 수의 점, 복잡하거나 세밀한 부분 : 더 많은 수의 점)
- 단점 :
    - 시각화 및 후처리 과정이 필요함
        - 포인트 클라우드 표현은 본질적으로 infinitesimally small point로 이뤄져있기 때문에 그대로는 우리 눈으로 볼 수 x. 시각화를 위해서는 각 점을 일정한 크기의 구로 확장해서 렌더링해야함.
- 활용 :
    - Neural Networks 에서 유용하게 사용됨. (자율주행 자동차 지붕의 LiDAR 센서는 포인트 클라우드 형태로 3D 정보 수집함) → 포인트 클라우드를 신경망이 처리해서 주변 환경을 인식하고 의사 결정을 수행해야함.
- PointNet :
    
    ![image.png](assets/img/250212post/image22.png)
    
    - 입력 : P개의 3D 점들(각각 x, y, z 좌표 포함)
    - 특징 :
        - 포인트 클라우드를 분석해 Classification or Regression 작업을 수행.
        - 포인트의 순서(order)에 영향을 받지 않아야함. (본질적으로 순서가 없는 집합이므로, 신경망이 이 입력 데이터를 처리할 때 순서에 영향을 받지 않도록 해야함. )
    - 구조 :
        - 각 포인트를 독립적으로 처리 : 다중 퍼셉트론(MLP)를 사용해 개별 점을 저차원 feature → 고차원 feature
        - Max Pooling 을 사용한 전역 정보 추출 : 점 개수에 상관없이 모든 점에 대해 맥스 풀링 연산을 수행해 전체 포인트 클라우드의 대표적인 특징 벡터 생성 (입력 포인트의 순서가 달라져도 모든 결과가 동일하게 유지되어 순서 불변성 보장. )
        - 출력 계층 : 최종적으로 이 특징 벡터를 사용하여 분류, 회귀 수행.

![image.png](assets/img/250212post/image23.png)

신경망을 활용하여 포인트 클라우드를 생성하는 문제. (RGB 이미지를 입력받아 해당 물체의  3D 포인트 클라우드 생성하는 모델) → Loss Function 필요. (예측 포인트 클라우드와 정답 포인트 클라우드 사이의 차이를 측정할 수 있는)

- 샴퍼 거리(Chamfer Distance) 손실함수
    - 포인트 클라우드간의 차이를 측정하는 대표적인 손실함수
    1. 예측한 포인트 클라우드(파란 점)와 정답 포인트 클라우드(주황 점)을 비교
    2. 각 파란 점에 대해 가장 가까운 주황 점을 찾고 두 점 사이의 유클리드 거리 합산
    3. 각 주황 점에 대해 가장 가까운 파란 점을 찾고 두 점 사이의 유클리드 거리 합산
    4. 위 두 값을 합산한게 최종 샴퍼 거리.
    - 포인트가 완전히 일치할때 거리가 0이됨. 최근접 이웃을 기준으로 거리 계산을 수행하므로 포인트의 순서가 달라도 동일한 값 가짐.

### 2.5 Mesh

![image.png](assets/img/250212post/image24.png)

- 개념 : 3D공간의  정점 집합을 사용해 형상을 표현. 정점들을 삼각형 단위로 연결하여 3D 형상의 표면을 구성.

![image.png](assets/img/250212post/image25.png)

- 장점 :
    - 메시를 구성하는 삼각형의 크기를 조절해 특정 영역을 더 정밀하게 표현 가능
    - 포인트 클라우드에서는 표면 정보가 직접적으로 정의되지 않지만, 삼각형 메시 방식에는 surface가 명확하게 정의됨.
    - 각 정점(vertex)에 데이터(색, 법선벡터, 텍스처 좌표 등) 저장 가능. 삼각형 면 내에서 interpolation을 통해 정점 사이의 데이터 연결 가능.
- 단점 :
    - Neural Network에서 처리하는 것은 어려움.
        
        → 포인트 클라우드는 순서가 없는 점들의 집합이지만 메시에서는 점 뿐만 아니라 삼각형 간의 연결 관계도 함께 고려해야함.
        
        → 메시 데이터는 단순한 배열이 아니라 그래프 형태로 되어있어 기존 신경망 적용 x
        
- **Pixel2Mesh**
    
    ![image.png](assets/img/250212post/image26.png)
    
    - **Interactive Mesh Refinement**
        
        ![image.png](assets/img/250212post/image27.png)
        
        - 3D 삼각형 메시를 처음부터 생성하기엔 어려우니까 기본 형태를 먼저 설정한 후, 점진적으로 Refinement 하는 방식
        - 신경망의 입력으로는 초기 메시(보통 구 또는 타원체 형태의 메시)를 사용하고, 이를 반복적인 변형(deformation)을 통해 최종 형태로 개선
        1. 초기 메시(Sphere or Ellipsoid)를 입력으로 설정. → 모든 정점과 삼각형을 포함하는 기본메시 사용
        2. 이미지와 비교하여 정점 이동 → 신경망이 메시의 형태를 분석하고 각 정점의 위치를 조정
        3. 여러 단계의 개선 과정 반복. → 메시가 더 정확한 3D 형상으로 변형. 
    - **Graph Convolution**
        
        ![image.png](assets/img/250212post/image28.png)
        
        - 삼각형 메시는 정점과 정점 사이의 연결로 이뤄진 그래프 형태이므로 Graph Convolution.
        1. 각 정점에 특징 벡터 할당
        2. 신경망이 이웃 정점들과의 관계를 고려해 새로운 특징 계산
        3. 이 연산을 메시 전체에 적용해 최종 3D 메시 생성
            
            ![image.png](assets/img/250212post/image29.png)
            
    - **Vertex-Aligned Features**
        
        ![image.png](assets/img/250212post/image30.png)
        
        - 입력이미지와 3D 메시를 결합하기 위해 정점 위치를 이미지 좌표로 변환해 특징 추출
        1. 입력 RGB이미지를 2D 합성곱 신경망에 통과시켜 Feature Map 생성
        2. 생성된 3D 메시의 각 정점을 2D 이미지 좌표로 변환(3D to 2D Projection)
        3. 변환된 좌표를 이용해 이미지 특징을 샘플링(Bilinear Interpolation)
        4. 최종적으로 이미지 정보를 포함한 메시를 신경망에 입력
        
        ![image.png](assets/img/250212post/image31.png)
        
    - **Chamfer Distance Loss**
        
        ![image.png](assets/img/250212post/image32.png)
        
        - 3D 삼각형 메시는 같은 형상을 표현하는 다양한 방식이 존재할 수 있음→ 예를 들어, 정사각형을 표현할 때 2개의 삼각형을 사용할 수도 있고, 4개의 삼각형을 사용할 수도 있음
        - 따라서 단순한 삼각형 비교 방식이 아니라, 형상의 본질적인 차이를 비교하는 손실 함수가 필요
        1. 예측된 메시와 정답 메시에서 랜덤하게 점(Points)을 샘플링
        2. 두 포인트 클라우드 간의 Chamfer Distance를 계산하여 손실값으로 사용
        3. 이를 통해 형상의 차이를 수치적으로 평가하고, 학습을 진행
        
        ![image.png](assets/img/250212post/image33.png)
        

## 3. Shape Comparison Metrics

![image.png](assets/img/250212post/image34.png)

### 3.1 **Intersection over Union**

![image.png](assets/img/250212post/image35.png)

- 3D 형상의 IoU는 우리가 기대하는 것만큼 의미 있거나 유용한 지표가 아님

### 3.2 **Chamfer Distance**

![image.png](assets/img/250212post/image36.png)

- L2 거리 기반 손실함수이기 때문에 이상치에 매우 민감.

### 3.3 **F1 Score (Best)**

![image.png](assets/img/250212post/image37.png)

- **F1 Score 계산방법**
    1. **정확도(Precision) 계산**
        1. 예측된 포인트 클라우드(주황)가 실제 정답 포인트 클라우드(파랑)과 얼마나 일치하는지를 계산.
        2. 예측된 포인트가 일정 반경 내에 정답 포인트를 포한하는 경우 정답으로 간주.
        3. 위의 예제에서 Precision은 3/4
    2. **재현율(Recall) 계산**
        1. 정답 포인트 클라우드(파랑) 중에서 예측포인트(주황)와 일치하는 비율
        2. 위의 예제에서 Recall은 2/3
    3. **F1 Score 계산**
        1. Precision과 Recall의 조화 평균을 계산.
        2. 위의 예제에서는 0.7
        3. 값의 범위는 0~1이고 Precision, Recall이 둘다 1일때 F1 score도 1
    
    F1 Score는 Chamfer Distance보다 이상치에 덜 민감하여, 3D 형상을 비교하는 데 더 적합한 지표
    

## **4. Cameras**

이미지를 입력받아 3D형상을 출력하는 모델을 만들고자 한다면 출력될 3D 형상의 좌표계를 어떻게 설정할 것인지 결정해야함. 

### 4.1  Canonical vs View Coordinates

![image.png](assets/img/250212post/image38.png)

- **정규 좌표계(Canonical Coordinate System)**
    - 객체의 종류별로 **고정된 기준 방향**(정면, 왼쪽, 위쪽, 오른쪽, 아래쪽 등)을 정하는 방식.
        
        예를 들어, 의자(Chair)를 예측하는 모델을 훈련한다고 하면,
        
        - `+Z` 방향은 항상 의자의 앞(front)
        - `+Y` 방향은 항상 위쪽(의자 좌석에서 위로 올라가는 방향)
        - `+X` 방향은 항상 오른쪽(right)으로 설정할 수 있음.
- **뷰 좌표계(View Coordinate System)**
    - 3D 형상을 입력 이미지에 맞춰 정렬된 좌표계(View-aligned coordinate system)로 표현하는 방식
    - 모델의 출력과 입력이 더 잘 정렬됨. (뷰 좌표계로학습된 모델이 테스트시 일반화가 더 잘되는 경향이 있음)